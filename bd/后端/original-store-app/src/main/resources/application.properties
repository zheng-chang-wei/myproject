spring.profiles.active=local

#kafka.consumer.zookeeper.connect=192.168.1.13:2181
#kafka.consumer.servers=192.168.1.14:9092,192.168.1.13:9092,192.168.1.12:9092
kafka.consumer.enable.auto.commit=true
kafka.consumer.session.timeout=6000
kafka.consumer.auto.commit.interval=100
kafka.consumer.auto.offset.reset=latest
kafka.consumer.topic=train-ground
kafka.consumer.group.register=original-store

kafka.consumer.concurrency=3
kafka.consumer.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer

#报文主题前缀
kafka.original.topic.prefix=message-

#原始报文父目录
store.file.container=rt
#原始报文文件扩展名
store.file.extension=.rt

#写文件频率，单位秒，默认为5
store.file.write.period=5
#文件切换时间间隔，单位小时，默认为1小时
store.file.interval=1
#切换文件定时器表达式，当前为每小时
store.file.create.period=0 0 0/1 * * ? 

#文件上传定时器表达式
store.file.upload.period=0 0 0/6 * * ? 

#hdfs平台
#store.hdfs.defaultFS=hdfs://192.168.1.14:8022
#大数据平台数据存储根目录
store.hdfs.root=/data
#hdfs运行的linux系统的用户名
store.hdfs.username=hdfs

data.record.topic=big-data-record