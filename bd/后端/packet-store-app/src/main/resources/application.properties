spring.profiles.active=test

kafka.producer.retries=10
kafka.producer.batch.size=4096
kafka.producer.linger=1
#5M
kafka.producer.buffer.memory=5242880
kafka.producer.acks=1
kafka.producer.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.producer.value.serializer=org.apache.kafka.common.serialization.ByteArraySerializer
#kafka.consumer.zookeeper.connect=192.168.1.13:2181
#kafka.consumer.servers=192.168.1.14:9092,192.168.1.13:9092,192.168.1.12:9092
kafka.consumer.enable.auto.commit=true
kafka.consumer.session.timeout=6000
kafka.consumer.auto.commit.interval=100
kafka.consumer.auto.offset.reset=latest
kafka.consumer.topic=train-ground
kafka.consumer.group.register=packet-store

kafka.consumer.concurrency=3
kafka.consumer.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
kafka.consumer.value.deserializer=org.apache.kafka.common.serialization.ByteArrayDeserializer

#报文主题前缀
kafka.original.topic.prefix=decode-message-

#大数据平台数据存储根目录
store.hdfs.root=/temp
#hdfs运行的linux系统的用户名
store.hdfs.username=hdfs

hive.url=jdbc:hive2://192.168.40.34:10000/test
hive.user=hive
hive.password=123456
hive.driver=org.apache.hive.jdbc.HiveDriver